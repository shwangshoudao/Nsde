{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================training the neural sde model==============================\n",
      "epoch: 0\n",
      "validation 0, loss=0.8951176404953003\n",
      "iteration 1, loss=0.8951176404953003\n",
      "iteration 2, loss=0.837317168712616\n",
      "iteration 3, loss=0.7755511403083801\n",
      "iteration 4, loss=0.7101556658744812\n",
      "iteration 5, loss=0.6418028473854065\n",
      "iteration 6, loss=0.5704524517059326\n",
      "iteration 7, loss=0.49670305848121643\n",
      "iteration 8, loss=0.4214025139808655\n",
      "iteration 9, loss=0.3457224369049072\n",
      "iteration 10, loss=0.272094190120697\n",
      "epoch: 1\n",
      "validation 10, loss=0.21853013336658478\n",
      "iteration 11, loss=0.21853013336658478\n",
      "iteration 12, loss=0.16543011367321014\n",
      "iteration 13, loss=0.15063562989234924\n",
      "iteration 14, loss=0.1726376712322235\n",
      "iteration 15, loss=0.20604877173900604\n",
      "iteration 16, loss=0.2324412763118744\n",
      "iteration 17, loss=0.2463664710521698\n",
      "iteration 18, loss=0.2478863000869751\n",
      "iteration 19, loss=0.2389422208070755\n",
      "iteration 20, loss=0.22221232950687408\n",
      "epoch: 2\n",
      "validation 20, loss=0.19267050921916962\n",
      "iteration 21, loss=0.19267050921916962\n",
      "iteration 22, loss=0.17246794700622559\n",
      "iteration 23, loss=0.15677005052566528\n",
      "iteration 24, loss=0.14944586157798767\n",
      "iteration 25, loss=0.15119126439094543\n",
      "iteration 26, loss=0.15875020623207092\n",
      "iteration 27, loss=0.16755583882331848\n",
      "iteration 28, loss=0.17430661618709564\n",
      "iteration 29, loss=0.17742682993412018\n",
      "iteration 30, loss=0.17661890387535095\n",
      "epoch: 3\n",
      "validation 30, loss=0.1795879751443863\n",
      "iteration 31, loss=0.1795879751443863\n",
      "iteration 32, loss=0.17119760811328888\n",
      "iteration 33, loss=0.1610732525587082\n",
      "iteration 34, loss=0.15142974257469177\n",
      "iteration 35, loss=0.14458675682544708\n",
      "iteration 36, loss=0.14277946949005127\n",
      "iteration 37, loss=0.14607545733451843\n",
      "iteration 38, loss=0.15082211792469025\n",
      "iteration 39, loss=0.15425258874893188\n",
      "iteration 40, loss=0.15546081960201263\n",
      "epoch: 4\n",
      "validation 40, loss=0.17764347791671753\n",
      "iteration 41, loss=0.17764347791671753\n",
      "iteration 42, loss=0.17309533059597015\n",
      "iteration 43, loss=0.16597634553909302\n",
      "iteration 44, loss=0.1584724485874176\n",
      "iteration 45, loss=0.15281492471694946\n",
      "iteration 46, loss=0.15047074854373932\n",
      "iteration 47, loss=0.15146246552467346\n",
      "iteration 48, loss=0.15448562800884247\n",
      "iteration 49, loss=0.15779058635234833\n",
      "iteration 50, loss=0.1599954217672348\n",
      "epoch: 5\n",
      "validation 50, loss=0.15937374532222748\n",
      "iteration 51, loss=0.15937374532222748\n",
      "iteration 52, loss=0.1577320694923401\n",
      "iteration 53, loss=0.15464358031749725\n",
      "iteration 54, loss=0.15116089582443237\n",
      "iteration 55, loss=0.14840467274188995\n",
      "iteration 56, loss=0.1471545398235321\n",
      "iteration 57, loss=0.147530198097229\n",
      "iteration 58, loss=0.14895939826965332\n",
      "iteration 59, loss=0.15053007006645203\n",
      "iteration 60, loss=0.15145234763622284\n",
      "epoch: 6\n",
      "validation 60, loss=0.15053869783878326\n",
      "iteration 61, loss=0.15053869783878326\n",
      "iteration 62, loss=0.14944937825202942\n",
      "iteration 63, loss=0.14784720540046692\n",
      "iteration 64, loss=0.14638084173202515\n",
      "iteration 65, loss=0.1455589383840561\n",
      "iteration 66, loss=0.1455361694097519\n",
      "iteration 67, loss=0.1460990607738495\n",
      "iteration 68, loss=0.1468246728181839\n",
      "iteration 69, loss=0.14731192588806152\n",
      "iteration 70, loss=0.14734043180942535\n",
      "epoch: 7\n",
      "validation 70, loss=0.1511031985282898\n",
      "iteration 71, loss=0.1511031985282898\n",
      "iteration 72, loss=0.14851348102092743\n",
      "iteration 73, loss=0.1452818661928177\n",
      "iteration 74, loss=0.1426582634449005\n",
      "iteration 75, loss=0.14154277741909027\n",
      "iteration 76, loss=0.14206388592720032\n",
      "iteration 77, loss=0.1435682624578476\n",
      "iteration 78, loss=0.14505727589130402\n",
      "iteration 79, loss=0.14574474096298218\n",
      "iteration 80, loss=0.14536146819591522\n",
      "epoch: 8\n",
      "validation 80, loss=0.1410035789012909\n",
      "iteration 81, loss=0.1410035789012909\n",
      "iteration 82, loss=0.1400947868824005\n",
      "iteration 83, loss=0.13956500589847565\n",
      "iteration 84, loss=0.13955259323120117\n",
      "iteration 85, loss=0.13992296159267426\n",
      "iteration 86, loss=0.1403900533914566\n",
      "iteration 87, loss=0.14067956805229187\n",
      "iteration 88, loss=0.1406557559967041\n",
      "iteration 89, loss=0.14035511016845703\n",
      "iteration 90, loss=0.139942929148674\n",
      "epoch: 9\n",
      "validation 90, loss=0.14623264968395233\n",
      "iteration 91, loss=0.14623264968395233\n",
      "iteration 92, loss=0.14635233581066132\n",
      "iteration 93, loss=0.14653988182544708\n",
      "iteration 94, loss=0.14667773246765137\n",
      "iteration 95, loss=0.14669397473335266\n",
      "iteration 96, loss=0.14659079909324646\n",
      "iteration 97, loss=0.14642758667469025\n",
      "iteration 98, loss=0.1462841033935547\n",
      "iteration 99, loss=0.14621730148792267\n",
      "iteration 100, loss=0.14623667299747467\n",
      "epoch: 10\n",
      "validation 100, loss=0.15654230117797852\n",
      "iteration 101, loss=0.15654230117797852\n",
      "iteration 102, loss=0.15364202857017517\n",
      "iteration 103, loss=0.1503588706254959\n",
      "iteration 104, loss=0.1482580453157425\n",
      "iteration 105, loss=0.14805221557617188\n",
      "iteration 106, loss=0.14931219816207886\n",
      "iteration 107, loss=0.15092606842517853\n",
      "iteration 108, loss=0.151866152882576\n",
      "iteration 109, loss=0.1516687273979187\n",
      "iteration 110, loss=0.1505153328180313\n",
      "epoch: 11\n",
      "validation 110, loss=0.16207602620124817\n",
      "iteration 111, loss=0.16207602620124817\n",
      "iteration 112, loss=0.15324203670024872\n",
      "iteration 113, loss=0.1436956226825714\n",
      "iteration 114, loss=0.13748638331890106\n",
      "iteration 115, loss=0.13714861869812012\n",
      "iteration 116, loss=0.14147670567035675\n",
      "iteration 117, loss=0.14663338661193848\n",
      "iteration 118, loss=0.1493408977985382\n",
      "iteration 119, loss=0.1484164148569107\n",
      "iteration 120, loss=0.14466316998004913\n",
      "epoch: 12\n",
      "validation 120, loss=0.15100538730621338\n",
      "iteration 121, loss=0.15100538730621338\n",
      "iteration 122, loss=0.14522375166416168\n",
      "iteration 123, loss=0.14252974092960358\n",
      "iteration 124, loss=0.14350192248821259\n",
      "iteration 125, loss=0.14654859900474548\n",
      "iteration 126, loss=0.149380624294281\n",
      "iteration 127, loss=0.15043963491916656\n",
      "iteration 128, loss=0.14937296509742737\n",
      "iteration 129, loss=0.14686468243598938\n",
      "iteration 130, loss=0.1441991627216339\n",
      "epoch: 13\n",
      "validation 130, loss=0.14450383186340332\n",
      "iteration 131, loss=0.14450383186340332\n",
      "iteration 132, loss=0.14611861109733582\n",
      "iteration 133, loss=0.14752420783042908\n",
      "iteration 134, loss=0.14785383641719818\n",
      "iteration 135, loss=0.1470005065202713\n",
      "iteration 136, loss=0.14552748203277588\n",
      "iteration 137, loss=0.14425063133239746\n",
      "iteration 138, loss=0.14375056326389313\n",
      "iteration 139, loss=0.14406710863113403\n",
      "iteration 140, loss=0.14477340877056122\n",
      "epoch: 14\n",
      "validation 140, loss=0.14932847023010254\n",
      "iteration 141, loss=0.14932847023010254\n",
      "iteration 142, loss=0.14904531836509705\n",
      "iteration 143, loss=0.1478094607591629\n",
      "iteration 144, loss=0.14633168280124664\n",
      "iteration 145, loss=0.14535954594612122\n",
      "iteration 146, loss=0.14526715874671936\n",
      "iteration 147, loss=0.14587806165218353\n",
      "iteration 148, loss=0.14664731919765472\n",
      "iteration 149, loss=0.14704382419586182\n",
      "iteration 150, loss=0.14685307443141937\n",
      "epoch: 15\n",
      "validation 150, loss=0.15738534927368164\n",
      "iteration 151, loss=0.15738534927368164\n",
      "iteration 152, loss=0.1543671041727066\n",
      "iteration 153, loss=0.15152540802955627\n",
      "iteration 154, loss=0.1502474993467331\n",
      "iteration 155, loss=0.15079711377620697\n",
      "iteration 156, loss=0.15233400464057922\n",
      "iteration 157, loss=0.15365460515022278\n",
      "iteration 158, loss=0.15395882725715637\n",
      "iteration 159, loss=0.15316128730773926\n",
      "iteration 160, loss=0.1517736166715622\n",
      "epoch: 16\n",
      "validation 160, loss=0.1502070128917694\n",
      "iteration 161, loss=0.1502070128917694\n",
      "iteration 162, loss=0.14832864701747894\n",
      "iteration 163, loss=0.14769050478935242\n",
      "iteration 164, loss=0.14831145107746124\n",
      "iteration 165, loss=0.14945575594902039\n",
      "iteration 166, loss=0.15020135045051575\n",
      "iteration 167, loss=0.15000058710575104\n",
      "iteration 168, loss=0.14899343252182007\n",
      "iteration 169, loss=0.14782920479774475\n",
      "iteration 170, loss=0.14717908203601837\n",
      "epoch: 17\n",
      "validation 170, loss=0.14447353780269623\n",
      "iteration 171, loss=0.14447353780269623\n",
      "iteration 172, loss=0.14401978254318237\n",
      "iteration 173, loss=0.14465181529521942\n",
      "iteration 174, loss=0.14518742263317108\n",
      "iteration 175, loss=0.14504683017730713\n",
      "iteration 176, loss=0.14459341764450073\n",
      "iteration 177, loss=0.1443362981081009\n",
      "iteration 178, loss=0.14418528974056244\n",
      "iteration 179, loss=0.14422176778316498\n",
      "iteration 180, loss=0.14435705542564392\n",
      "epoch: 18\n",
      "validation 180, loss=0.14748945832252502\n",
      "iteration 181, loss=0.14748945832252502\n",
      "iteration 182, loss=0.14711032807826996\n",
      "iteration 183, loss=0.14643888175487518\n",
      "iteration 184, loss=0.14572758972644806\n",
      "iteration 185, loss=0.1451907902956009\n",
      "iteration 186, loss=0.1448725461959839\n",
      "iteration 187, loss=0.14449431002140045\n",
      "iteration 188, loss=0.14390288293361664\n",
      "iteration 189, loss=0.14311377704143524\n",
      "iteration 190, loss=0.14215031266212463\n",
      "epoch: 19\n",
      "validation 190, loss=0.14301349222660065\n",
      "iteration 191, loss=0.14301349222660065\n",
      "iteration 192, loss=0.14244425296783447\n",
      "iteration 193, loss=0.14169813692569733\n",
      "iteration 194, loss=0.14095038175582886\n",
      "iteration 195, loss=0.1402330845594406\n",
      "iteration 196, loss=0.13965019583702087\n",
      "iteration 197, loss=0.1390637755393982\n",
      "iteration 198, loss=0.13841812312602997\n",
      "iteration 199, loss=0.13765612244606018\n",
      "iteration 200, loss=0.13690496981143951\n",
      "epoch: 20\n",
      "validation 200, loss=0.1376316398382187\n",
      "iteration 201, loss=0.1376316398382187\n",
      "iteration 202, loss=0.137399822473526\n",
      "iteration 203, loss=0.13712121546268463\n",
      "iteration 204, loss=0.13683663308620453\n",
      "iteration 205, loss=0.13654105365276337\n",
      "iteration 206, loss=0.1363087147474289\n",
      "iteration 207, loss=0.13609984517097473\n",
      "iteration 208, loss=0.1358724683523178\n",
      "iteration 209, loss=0.13561439514160156\n",
      "iteration 210, loss=0.1353473961353302\n",
      "epoch: 21\n",
      "validation 210, loss=0.15042796730995178\n",
      "iteration 211, loss=0.15042796730995178\n",
      "iteration 212, loss=0.14992690086364746\n",
      "iteration 213, loss=0.1493697166442871\n",
      "iteration 214, loss=0.14907453954219818\n",
      "iteration 215, loss=0.14893482625484467\n",
      "iteration 216, loss=0.14861352741718292\n",
      "iteration 217, loss=0.14796729385852814\n",
      "iteration 218, loss=0.14716196060180664\n",
      "iteration 219, loss=0.14643031358718872\n",
      "iteration 220, loss=0.1459292471408844\n",
      "epoch: 22\n",
      "validation 220, loss=0.14008864760398865\n",
      "iteration 221, loss=0.14008864760398865\n",
      "iteration 222, loss=0.13931404054164886\n",
      "iteration 223, loss=0.1379307508468628\n",
      "iteration 224, loss=0.13671346008777618\n",
      "iteration 225, loss=0.13612625002861023\n",
      "iteration 226, loss=0.13588377833366394\n",
      "iteration 227, loss=0.13545122742652893\n",
      "iteration 228, loss=0.13486522436141968\n",
      "iteration 229, loss=0.13455206155776978\n",
      "iteration 230, loss=0.1346539855003357\n",
      "epoch: 23\n",
      "validation 230, loss=0.185244619846344\n",
      "iteration 231, loss=0.185244619846344\n",
      "iteration 232, loss=0.18109126389026642\n",
      "iteration 233, loss=0.17556270956993103\n",
      "iteration 234, loss=0.1720658838748932\n",
      "iteration 235, loss=0.1705273538827896\n",
      "iteration 236, loss=0.16880865395069122\n",
      "iteration 237, loss=0.16571658849716187\n",
      "iteration 238, loss=0.16180956363677979\n",
      "iteration 239, loss=0.15871526300907135\n",
      "iteration 240, loss=0.15692196786403656\n",
      "epoch: 24\n",
      "validation 240, loss=0.14659762382507324\n",
      "iteration 241, loss=0.14659762382507324\n",
      "iteration 242, loss=0.1452929675579071\n",
      "iteration 243, loss=0.1453007310628891\n",
      "iteration 244, loss=0.14617739617824554\n",
      "iteration 245, loss=0.1470838487148285\n",
      "iteration 246, loss=0.14732703566551208\n",
      "iteration 247, loss=0.14686527848243713\n",
      "iteration 248, loss=0.1461128443479538\n",
      "iteration 249, loss=0.1455608308315277\n",
      "iteration 250, loss=0.14544333517551422\n",
      "epoch: 25\n",
      "validation 250, loss=0.13752713799476624\n",
      "iteration 251, loss=0.13752713799476624\n",
      "iteration 252, loss=0.13758407533168793\n",
      "iteration 253, loss=0.13698486983776093\n",
      "iteration 254, loss=0.13607360422611237\n",
      "iteration 255, loss=0.1353311985731125\n",
      "iteration 256, loss=0.13491849601268768\n",
      "iteration 257, loss=0.13463431596755981\n",
      "iteration 258, loss=0.13415709137916565\n",
      "iteration 259, loss=0.13336065411567688\n",
      "iteration 260, loss=0.13251879811286926\n",
      "epoch: 26\n",
      "validation 260, loss=0.13817048072814941\n",
      "iteration 261, loss=0.13817048072814941\n",
      "iteration 262, loss=0.13718733191490173\n",
      "iteration 263, loss=0.13692082464694977\n",
      "iteration 264, loss=0.1372625231742859\n",
      "iteration 265, loss=0.13757136464118958\n",
      "iteration 266, loss=0.13737064599990845\n",
      "iteration 267, loss=0.13688337802886963\n",
      "iteration 268, loss=0.13659155368804932\n",
      "iteration 269, loss=0.13675768673419952\n",
      "iteration 270, loss=0.1371341198682785\n",
      "epoch: 27\n",
      "validation 270, loss=0.16450363397598267\n",
      "iteration 271, loss=0.16450363397598267\n",
      "iteration 272, loss=0.16105565428733826\n",
      "iteration 273, loss=0.15636678040027618\n",
      "iteration 274, loss=0.15363340079784393\n",
      "iteration 275, loss=0.15332169830799103\n",
      "iteration 276, loss=0.15349194407463074\n",
      "iteration 277, loss=0.15245908498764038\n",
      "iteration 278, loss=0.1497706174850464\n",
      "iteration 279, loss=0.146521657705307\n",
      "iteration 280, loss=0.14389152824878693\n",
      "epoch: 28\n",
      "validation 280, loss=0.14484286308288574\n",
      "iteration 281, loss=0.14484286308288574\n",
      "iteration 282, loss=0.14537645876407623\n",
      "iteration 283, loss=0.14595115184783936\n",
      "iteration 284, loss=0.14593520760536194\n",
      "iteration 285, loss=0.14668896794319153\n",
      "iteration 286, loss=0.14687329530715942\n",
      "iteration 287, loss=0.14679135382175446\n",
      "iteration 288, loss=0.1472114771604538\n",
      "iteration 289, loss=0.1472584456205368\n",
      "iteration 290, loss=0.1469135582447052\n",
      "epoch: 29\n",
      "validation 290, loss=0.15528233349323273\n",
      "iteration 291, loss=0.15528233349323273\n",
      "iteration 292, loss=0.15395385026931763\n",
      "iteration 293, loss=0.15224897861480713\n",
      "iteration 294, loss=0.15129581093788147\n",
      "iteration 295, loss=0.15064527094364166\n",
      "iteration 296, loss=0.14976570010185242\n",
      "iteration 297, loss=0.14854489266872406\n",
      "iteration 298, loss=0.14725710451602936\n",
      "iteration 299, loss=0.1462896764278412\n",
      "iteration 300, loss=0.1458231657743454\n",
      "******************** The calibrated heston model params:         rho = 0.75,theta = 0.89, kappa = 2.23 and lambda = 0.3 ********************\n",
      "==============================training the neural sde pro model==============================\n",
      "epoch: 0\n",
      "validation 0, loss=17.2999210357666\n",
      "iteration 1, loss=17.2999210357666\n",
      "iteration 2, loss=15.416305541992188\n",
      "iteration 3, loss=13.609987258911133\n",
      "iteration 4, loss=11.813077926635742\n",
      "iteration 5, loss=10.08420467376709\n",
      "iteration 6, loss=8.41930103302002\n",
      "iteration 7, loss=6.835936546325684\n",
      "iteration 8, loss=5.352483749389648\n",
      "iteration 9, loss=4.004667282104492\n",
      "iteration 10, loss=2.827876567840576\n",
      "epoch: 1\n",
      "validation 10, loss=1.6629687547683716\n",
      "iteration 11, loss=1.6629687547683716\n",
      "iteration 12, loss=0.962460994720459\n",
      "iteration 13, loss=0.5419421792030334\n",
      "iteration 14, loss=0.4664858877658844\n",
      "iteration 15, loss=0.5673075318336487\n",
      "iteration 16, loss=0.6506783962249756\n",
      "iteration 17, loss=0.6960955858230591\n",
      "iteration 18, loss=0.7111918330192566\n",
      "iteration 19, loss=0.7028632760047913\n",
      "iteration 20, loss=0.674636960029602\n",
      "epoch: 2\n",
      "validation 20, loss=0.6360756754875183\n",
      "iteration 21, loss=0.6360756754875183\n",
      "iteration 22, loss=0.5757834315299988\n",
      "iteration 23, loss=0.5025923252105713\n",
      "iteration 24, loss=0.41993245482444763\n",
      "iteration 25, loss=0.3294453024864197\n",
      "iteration 26, loss=0.23195718228816986\n",
      "iteration 27, loss=0.13753578066825867\n",
      "iteration 28, loss=0.10099628567695618\n",
      "iteration 29, loss=0.16007868945598602\n",
      "iteration 30, loss=0.22946520149707794\n",
      "epoch: 3\n",
      "validation 30, loss=0.2639509439468384\n",
      "iteration 31, loss=0.2639509439468384\n",
      "iteration 32, loss=0.2906767725944519\n",
      "iteration 33, loss=0.2972469925880432\n",
      "iteration 34, loss=0.28541868925094604\n",
      "iteration 35, loss=0.2572581470012665\n",
      "iteration 36, loss=0.216518372297287\n",
      "iteration 37, loss=0.16821971535682678\n",
      "iteration 38, loss=0.12352527678012848\n",
      "iteration 39, loss=0.10714712738990784\n",
      "iteration 40, loss=0.12867215275764465\n",
      "epoch: 4\n",
      "validation 40, loss=0.14208252727985382\n",
      "iteration 41, loss=0.14208252727985382\n",
      "iteration 42, loss=0.161361962556839\n",
      "iteration 43, loss=0.16808798909187317\n",
      "iteration 44, loss=0.1622549146413803\n",
      "iteration 45, loss=0.14702428877353668\n",
      "iteration 46, loss=0.12873724102973938\n",
      "iteration 47, loss=0.11733055114746094\n",
      "iteration 48, loss=0.11876866221427917\n",
      "iteration 49, loss=0.12749280035495758\n",
      "iteration 50, loss=0.1339089572429657\n",
      "epoch: 5\n",
      "validation 50, loss=0.1411692500114441\n",
      "iteration 51, loss=0.1411692500114441\n",
      "iteration 52, loss=0.1317378133535385\n",
      "iteration 53, loss=0.1181270182132721\n",
      "iteration 54, loss=0.109053835272789\n",
      "iteration 55, loss=0.11058744043111801\n",
      "iteration 56, loss=0.11857044696807861\n",
      "iteration 57, loss=0.12512411177158356\n",
      "iteration 58, loss=0.1255733221769333\n",
      "iteration 59, loss=0.11994048207998276\n",
      "iteration 60, loss=0.11157377809286118\n",
      "epoch: 6\n",
      "validation 60, loss=0.11152531951665878\n",
      "iteration 61, loss=0.11152531951665878\n",
      "iteration 62, loss=0.10481415688991547\n",
      "iteration 63, loss=0.10637455433607101\n",
      "iteration 64, loss=0.11255444586277008\n",
      "iteration 65, loss=0.11693953722715378\n",
      "iteration 66, loss=0.11633669584989548\n",
      "iteration 67, loss=0.11147859692573547\n",
      "iteration 68, loss=0.10596849024295807\n",
      "iteration 69, loss=0.10362587869167328\n",
      "iteration 70, loss=0.10534237325191498\n",
      "epoch: 7\n",
      "validation 70, loss=0.11495036631822586\n",
      "iteration 71, loss=0.11495036631822586\n",
      "iteration 72, loss=0.11490840464830399\n",
      "iteration 73, loss=0.10984809696674347\n",
      "iteration 74, loss=0.1036188006401062\n",
      "iteration 75, loss=0.10100653767585754\n",
      "iteration 76, loss=0.10320298373699188\n",
      "iteration 77, loss=0.10669859498739243\n",
      "iteration 78, loss=0.10770945996046066\n",
      "iteration 79, loss=0.10530983656644821\n",
      "iteration 80, loss=0.10152634978294373\n",
      "epoch: 8\n",
      "validation 80, loss=0.10302823036909103\n",
      "iteration 81, loss=0.10302823036909103\n",
      "iteration 82, loss=0.10415918380022049\n",
      "iteration 83, loss=0.10584385693073273\n",
      "iteration 84, loss=0.10603965073823929\n",
      "iteration 85, loss=0.10445598512887955\n",
      "iteration 86, loss=0.10242010653018951\n",
      "iteration 87, loss=0.10154901444911957\n",
      "iteration 88, loss=0.10217619687318802\n",
      "iteration 89, loss=0.10318098962306976\n",
      "iteration 90, loss=0.1033419668674469\n",
      "epoch: 9\n",
      "validation 90, loss=0.10189585387706757\n",
      "iteration 91, loss=0.10189585387706757\n",
      "iteration 92, loss=0.09927528351545334\n",
      "iteration 93, loss=0.09769091755151749\n",
      "iteration 94, loss=0.09813866019248962\n",
      "iteration 95, loss=0.09946348518133163\n",
      "iteration 96, loss=0.09986550360918045\n",
      "iteration 97, loss=0.09883834421634674\n",
      "iteration 98, loss=0.09736127406358719\n",
      "iteration 99, loss=0.09677745401859283\n",
      "iteration 100, loss=0.09730197489261627\n",
      "epoch: 10\n",
      "validation 100, loss=0.11676763743162155\n",
      "iteration 101, loss=0.11676763743162155\n",
      "iteration 102, loss=0.11057481169700623\n",
      "iteration 103, loss=0.1000790223479271\n",
      "iteration 104, loss=0.0938238650560379\n",
      "iteration 105, loss=0.09611973911523819\n",
      "iteration 106, loss=0.10104195773601532\n",
      "iteration 107, loss=0.1018126830458641\n",
      "iteration 108, loss=0.09763015061616898\n",
      "iteration 109, loss=0.09291023761034012\n",
      "iteration 110, loss=0.09267351031303406\n",
      "epoch: 11\n",
      "validation 110, loss=0.1166113018989563\n",
      "iteration 111, loss=0.1166113018989563\n",
      "iteration 112, loss=0.10905873030424118\n",
      "iteration 113, loss=0.10596442222595215\n",
      "iteration 114, loss=0.10686177760362625\n",
      "iteration 115, loss=0.1068110466003418\n",
      "iteration 116, loss=0.10270565003156662\n",
      "iteration 117, loss=0.09610477834939957\n",
      "iteration 118, loss=0.09209144115447998\n",
      "iteration 119, loss=0.09390994906425476\n",
      "iteration 120, loss=0.09850437194108963\n",
      "epoch: 12\n",
      "validation 120, loss=0.10347161442041397\n",
      "iteration 121, loss=0.10347161442041397\n",
      "iteration 122, loss=0.10439501702785492\n",
      "iteration 123, loss=0.10380591452121735\n",
      "iteration 124, loss=0.10218742489814758\n",
      "iteration 125, loss=0.10020200163125992\n",
      "iteration 126, loss=0.09830892831087112\n",
      "iteration 127, loss=0.09674908220767975\n",
      "iteration 128, loss=0.09568916261196136\n",
      "iteration 129, loss=0.09530849009752274\n",
      "iteration 130, loss=0.09565260261297226\n",
      "epoch: 13\n",
      "validation 130, loss=0.09417439997196198\n",
      "iteration 131, loss=0.09417439997196198\n",
      "iteration 132, loss=0.09300951659679413\n",
      "iteration 133, loss=0.09305909276008606\n",
      "iteration 134, loss=0.09420455247163773\n",
      "iteration 135, loss=0.09500382095575333\n",
      "iteration 136, loss=0.09455681592226028\n",
      "iteration 137, loss=0.09337493777275085\n",
      "iteration 138, loss=0.09260652214288712\n",
      "iteration 139, loss=0.09266838431358337\n",
      "iteration 140, loss=0.0929378867149353\n",
      "epoch: 14\n",
      "validation 140, loss=0.09674659371376038\n",
      "iteration 141, loss=0.09674659371376038\n",
      "iteration 142, loss=0.09640292078256607\n",
      "iteration 143, loss=0.09622897207736969\n",
      "iteration 144, loss=0.09621976315975189\n",
      "iteration 145, loss=0.09632430970668793\n",
      "iteration 146, loss=0.09646803140640259\n",
      "iteration 147, loss=0.09658180922269821\n",
      "iteration 148, loss=0.09661830216646194\n",
      "iteration 149, loss=0.09655997902154922\n",
      "iteration 150, loss=0.09642490744590759\n",
      "epoch: 15\n",
      "validation 150, loss=0.1027640849351883\n",
      "iteration 151, loss=0.1027640849351883\n",
      "iteration 152, loss=0.0998808965086937\n",
      "iteration 153, loss=0.09737787395715714\n",
      "iteration 154, loss=0.09711863100528717\n",
      "iteration 155, loss=0.09806729853153229\n",
      "iteration 156, loss=0.09822577238082886\n",
      "iteration 157, loss=0.09729669988155365\n",
      "iteration 158, loss=0.09663804620504379\n",
      "iteration 159, loss=0.09720303118228912\n",
      "iteration 160, loss=0.0982317253947258\n",
      "epoch: 16\n",
      "validation 160, loss=0.10160115361213684\n",
      "iteration 161, loss=0.10160115361213684\n",
      "iteration 162, loss=0.10108441859483719\n",
      "iteration 163, loss=0.10032808780670166\n",
      "iteration 164, loss=0.09955085068941116\n",
      "iteration 165, loss=0.09895288199186325\n",
      "iteration 166, loss=0.09866592288017273\n",
      "iteration 167, loss=0.0987103208899498\n",
      "iteration 168, loss=0.09898301213979721\n",
      "iteration 169, loss=0.09931831061840057\n",
      "iteration 170, loss=0.09956679493188858\n",
      "epoch: 17\n",
      "validation 170, loss=0.09456721693277359\n",
      "iteration 171, loss=0.09456721693277359\n",
      "iteration 172, loss=0.09334671497344971\n",
      "iteration 173, loss=0.091820627450943\n",
      "iteration 174, loss=0.09098094701766968\n",
      "iteration 175, loss=0.09064044058322906\n",
      "iteration 176, loss=0.09021717309951782\n",
      "iteration 177, loss=0.0898265689611435\n",
      "iteration 178, loss=0.09001030027866364\n",
      "iteration 179, loss=0.09073232859373093\n",
      "iteration 180, loss=0.09128575026988983\n",
      "epoch: 18\n",
      "validation 180, loss=0.09657693654298782\n",
      "iteration 181, loss=0.09657693654298782\n",
      "iteration 182, loss=0.09488886594772339\n",
      "iteration 183, loss=0.09397080540657043\n",
      "iteration 184, loss=0.09451504796743393\n",
      "iteration 185, loss=0.09508436173200607\n",
      "iteration 186, loss=0.09456954151391983\n",
      "iteration 187, loss=0.09365348517894745\n",
      "iteration 188, loss=0.093584805727005\n",
      "iteration 189, loss=0.09430471062660217\n",
      "iteration 190, loss=0.0946950912475586\n",
      "epoch: 19\n",
      "validation 190, loss=0.09623090922832489\n",
      "iteration 191, loss=0.09623090922832489\n",
      "iteration 192, loss=0.09472630172967911\n",
      "iteration 193, loss=0.09414347261190414\n",
      "iteration 194, loss=0.09455955028533936\n",
      "iteration 195, loss=0.09465385973453522\n",
      "iteration 196, loss=0.09395026415586472\n",
      "iteration 197, loss=0.09337791800498962\n",
      "iteration 198, loss=0.09371183067560196\n",
      "iteration 199, loss=0.094402015209198\n",
      "iteration 200, loss=0.09450030326843262\n",
      "epoch: 20\n",
      "validation 200, loss=0.09575963020324707\n",
      "iteration 201, loss=0.09575963020324707\n",
      "iteration 202, loss=0.09572941809892654\n",
      "iteration 203, loss=0.09549661725759506\n",
      "iteration 204, loss=0.09514381736516953\n",
      "iteration 205, loss=0.094936802983284\n",
      "iteration 206, loss=0.09498487412929535\n",
      "iteration 207, loss=0.09513121098279953\n",
      "iteration 208, loss=0.09519992768764496\n",
      "iteration 209, loss=0.09519393742084503\n",
      "iteration 210, loss=0.09520187973976135\n",
      "epoch: 21\n",
      "validation 210, loss=0.09762141108512878\n",
      "iteration 211, loss=0.09762141108512878\n",
      "iteration 212, loss=0.09537921100854874\n",
      "iteration 213, loss=0.09339608252048492\n",
      "iteration 214, loss=0.0939502865076065\n",
      "iteration 215, loss=0.09522566199302673\n",
      "iteration 216, loss=0.09482820332050323\n",
      "iteration 217, loss=0.09333184361457825\n",
      "iteration 218, loss=0.09294795989990234\n",
      "iteration 219, loss=0.09398911148309708\n",
      "iteration 220, loss=0.09464383870363235\n",
      "epoch: 22\n",
      "validation 220, loss=0.09955432265996933\n",
      "iteration 221, loss=0.09955432265996933\n",
      "iteration 222, loss=0.09742135554552078\n",
      "iteration 223, loss=0.09710802882909775\n",
      "iteration 224, loss=0.09851289540529251\n",
      "iteration 225, loss=0.09912905842065811\n",
      "iteration 226, loss=0.09807436913251877\n",
      "iteration 227, loss=0.09696952253580093\n",
      "iteration 228, loss=0.09726026654243469\n",
      "iteration 229, loss=0.09813851118087769\n",
      "iteration 230, loss=0.09812749177217484\n",
      "epoch: 23\n",
      "validation 230, loss=0.10412183403968811\n",
      "iteration 231, loss=0.10412183403968811\n",
      "iteration 232, loss=0.10046438872814178\n",
      "iteration 233, loss=0.09562971442937851\n",
      "iteration 234, loss=0.09514462947845459\n",
      "iteration 235, loss=0.09724617004394531\n",
      "iteration 236, loss=0.09715037792921066\n",
      "iteration 237, loss=0.09488843381404877\n",
      "iteration 238, loss=0.09423957765102386\n",
      "iteration 239, loss=0.0961933508515358\n",
      "iteration 240, loss=0.09750799834728241\n",
      "epoch: 24\n",
      "validation 240, loss=0.10063467174768448\n",
      "iteration 241, loss=0.10063467174768448\n",
      "iteration 242, loss=0.0989413633942604\n",
      "iteration 243, loss=0.09633206576108932\n",
      "iteration 244, loss=0.09464709460735321\n",
      "iteration 245, loss=0.0943480059504509\n",
      "iteration 246, loss=0.09451441466808319\n",
      "iteration 247, loss=0.09465161710977554\n",
      "iteration 248, loss=0.09511985629796982\n",
      "iteration 249, loss=0.09593840688467026\n",
      "iteration 250, loss=0.09635598212480545\n",
      "epoch: 25\n",
      "validation 250, loss=0.09732381254434586\n",
      "iteration 251, loss=0.09732381254434586\n",
      "iteration 252, loss=0.09535304456949234\n",
      "iteration 253, loss=0.09453228861093521\n",
      "iteration 254, loss=0.09530242532491684\n",
      "iteration 255, loss=0.09577067196369171\n",
      "iteration 256, loss=0.09511081874370575\n",
      "iteration 257, loss=0.09454168379306793\n",
      "iteration 258, loss=0.09501194208860397\n",
      "iteration 259, loss=0.09565583616495132\n",
      "iteration 260, loss=0.09540218859910965\n",
      "epoch: 26\n",
      "validation 260, loss=0.09120199829339981\n",
      "iteration 261, loss=0.09120199829339981\n",
      "iteration 262, loss=0.09145814925432205\n",
      "iteration 263, loss=0.09139504283666611\n",
      "iteration 264, loss=0.09114489704370499\n",
      "iteration 265, loss=0.09114325791597366\n",
      "iteration 266, loss=0.09134893119335175\n",
      "iteration 267, loss=0.0913793221116066\n",
      "iteration 268, loss=0.0911855697631836\n",
      "iteration 269, loss=0.09107346087694168\n",
      "iteration 270, loss=0.0911564826965332\n",
      "epoch: 27\n",
      "validation 270, loss=0.10708987712860107\n",
      "iteration 271, loss=0.10708987712860107\n",
      "iteration 272, loss=0.09813964366912842\n",
      "iteration 273, loss=0.09238075464963913\n",
      "iteration 274, loss=0.09647064656019211\n",
      "iteration 275, loss=0.100010447204113\n",
      "iteration 276, loss=0.09665510803461075\n",
      "iteration 277, loss=0.09211284667253494\n",
      "iteration 278, loss=0.09383300691843033\n",
      "iteration 279, loss=0.09774341434240341\n",
      "iteration 280, loss=0.09701564162969589\n",
      "epoch: 28\n",
      "validation 280, loss=0.09304545819759369\n",
      "iteration 281, loss=0.09304545819759369\n",
      "iteration 282, loss=0.09276394546031952\n",
      "iteration 283, loss=0.09399466961622238\n",
      "iteration 284, loss=0.0934598371386528\n",
      "iteration 285, loss=0.09173412621021271\n",
      "iteration 286, loss=0.09170177578926086\n",
      "iteration 287, loss=0.09322599321603775\n",
      "iteration 288, loss=0.0936640277504921\n",
      "iteration 289, loss=0.09253552556037903\n",
      "iteration 290, loss=0.09178260713815689\n",
      "epoch: 29\n",
      "validation 290, loss=0.09953885525465012\n",
      "iteration 291, loss=0.09953885525465012\n",
      "iteration 292, loss=0.09611646831035614\n",
      "iteration 293, loss=0.09152072668075562\n",
      "iteration 294, loss=0.09284552186727524\n",
      "iteration 295, loss=0.09604300558567047\n",
      "iteration 296, loss=0.09489233046770096\n",
      "iteration 297, loss=0.09129191190004349\n",
      "iteration 298, loss=0.09102793037891388\n",
      "iteration 299, loss=0.09355900436639786\n",
      "iteration 300, loss=0.09393821656703949\n",
      "The train loss for nsde:     tensor(0.0937)\n",
      "The train loss for nsde pro:     tensor(0.0651)\n",
      "tensor(20.8460)\n",
      "tensor(20.7371)\n",
      "The test loss for nsde:   tensor(44.1922)\n",
      "The test loss for nsde pro:   tensor(43.9930)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from scipy.optimize import least_squares\n",
    "import setenv\n",
    "from matplotlib import pyplot as plt\n",
    "from model import Net_SDE,Net_SDE_Pro\n",
    "from generate import heston\n",
    "\n",
    "\n",
    "# training function\n",
    "def train_models(model,target,n_steps,option_info,indices,MC_samples,seedused=1):\n",
    "    \"\"\"train nsde model\n",
    "\n",
    "    Args:\n",
    "        model (torch.model): nsde model  \n",
    "        target (np.numpy): 真实期权价格，用于做训练\n",
    "        n_steps (int): euler scheme的步数\n",
    "        option_info (list): 存option_info的数组, 0. 资产价格S0 1. 初始波动率V0 2.无风险利率rate 3.heston模型中的相关系数rho\n",
    "        indices (list): 到期日\n",
    "        MC_samples (int): 做仿真的sample数\n",
    "        seedused (int, optional): 随机种子. Defaults to 1.\n",
    "    \"\"\"\n",
    "    S0 = option_info[0]\n",
    "    V0 = option_info[1]\n",
    "    rate = option_info[2]\n",
    "    rho = option_info[3]\n",
    "    loss_fn = nn.MSELoss() \n",
    "    seedused=seedused+1\n",
    "    torch.manual_seed(seedused)\n",
    "    np.random.seed(seedused)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001, eps=1e-08,amsgrad=False,betas=(0.9, 0.999), weight_decay=0 )\n",
    "  # optimizer= torch.optim.Rprop(model.parameters(), lr=0.001, etas=(0.5, 1.2), step_sizes=(1e-07, 1))\n",
    "    n_epochs = 30\n",
    "    itercount = 0\n",
    "    losses_val = [] # for recording the loss val each epoch\n",
    "    losses = [] # for recording the loss each small batch\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "    # fix the seeds for reproducibility\n",
    "        np.random.seed(epoch+seedused*1000)\n",
    "        z_1 = np.random.normal(size=(MC_samples, n_steps))\n",
    "        z_2 = np.random.normal(size=(MC_samples, n_steps))\n",
    "        z_1 = np.append(z_1,-z_1,axis=0)\n",
    "        z_2 = np.append(z_2,-z_2,axis=0)\n",
    "        z_2  = rho*z_1+np.sqrt(1-rho ** 2)*z_2\n",
    "        z_1 = torch.tensor(z_1).to(device=device).float()\n",
    "        z_2 = torch.tensor(z_2).to(device=device).float()\n",
    "\n",
    "        print('epoch:', epoch)\n",
    "        \n",
    "#evaluate and print RMSE validation error at the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(strikes_call,strikes_put, indices, z_1,z_2, 2*MC_samples).detach()\n",
    "        loss_val=torch.sqrt(loss_fn(pred, target))\n",
    "        print('validation {}, loss={}'.format(itercount, loss_val.item()))\n",
    "\n",
    "#store the erorr value\n",
    "\n",
    "        losses_val.append(loss_val.clone().detach())\n",
    "        batch_size = 500\n",
    "# randomly reshufle samples and then use subsamples for training\n",
    "# this is useful when we want to reuse samples for each epoch\n",
    "        permutation = torch.randperm(int(2*MC_samples))\n",
    "        for i in range(0,2*MC_samples, batch_size):\n",
    "            indices2 = permutation[i:i+batch_size]\n",
    "            batch_x = z_1[indices2,:]\n",
    "            batch_y = z_2[indices2,:]         \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(strikes_call,strikes_put, indices, z_1,z_2, 2*MC_samples)\n",
    "            loss=torch.sqrt(loss_fn(pred, target))\n",
    "            losses.append(loss.clone().detach())\n",
    "            itercount += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('iteration {}, loss={}'.format(itercount, loss.item()))\n",
    "        \n",
    "            \n",
    "    return model,losses_val,losses  \n",
    "\n",
    "\n",
    "## setting and training\n",
    "\n",
    "device='cpu'\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "path = os.path.abspath(os.getcwd())\n",
    "model_path = path + \"/result_model/\"\n",
    "picture_path = path + \"/result_picture/\"\n",
    "data_path = path + \"/data/\"\n",
    "loss_path = path + \"/result_loss/\"\n",
    "\n",
    "ITM_call= torch.Tensor(torch.load(data_path+'Call_ITM_VG_train.pt')).to(device=device)\n",
    "ITM_put= torch.Tensor(torch.load(data_path+'Put_ITM_VG_train.pt')).to(device=device)\n",
    "OTM_call= torch.Tensor(torch.load(data_path+'Call_OTM_VG_train.pt')).to(device=device)\n",
    "OTM_put = torch.Tensor(torch.load(data_path+'Put_OTM_VG_train.pt')).to(device=device)\n",
    "\n",
    "## settings\n",
    "MC_samples = 2500\n",
    "strikes_put=np.arange(60, 101, 5).tolist()\n",
    "strikes_call=np.arange(100, 141, 5).tolist()\n",
    "S0 = torch.ones(1, 1)*100\n",
    "V0 = torch.ones(1,1)*0.04\n",
    "rate = torch.ones(1, 1)*0.032\n",
    "asset_info = [S0,V0,rate]\n",
    "rho = -0.7\n",
    "option_info = [S0,V0,rate,rho]\n",
    "n_steps = 360\n",
    "# generate subdivisions of 1 year interval\n",
    "timegrid = torch.linspace(0,1,n_steps+1) \n",
    "# If using n_steps=48 those corresponds to monthly maturities:\n",
    "indices = torch.tensor([30,60,90,120,150,180,240,270,300,360])  \n",
    "target=torch.cat([OTM_call, ITM_call],0)\n",
    "\n",
    "model = Net_SDE(asset_info = asset_info, n_dim = 3,timegrid = timegrid,\n",
    "                n_layers= 2,vNetWidth = 20,device = device)\n",
    "\n",
    "print(\"===\"*10+\"training the neural sde model\"+\"===\"*10)\n",
    "model,losses, losses_val=train_models(model,target,n_steps,option_info ,indices,MC_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# first step: calibrate the heston model\n",
    "\n",
    "S0 = 100\n",
    "V0 = 0.04\n",
    "r = 0.032\n",
    "\n",
    "strikes_all = np.append(strikes_call, strikes_put)\n",
    "indices = torch.tensor([30,60,90,120,150,180,240,270,300,360])  \n",
    "asset_input = []\n",
    "for j in range(len(indices)):\n",
    "    for i in range(len(strikes_all)):\n",
    "        asset_input.append([strikes_all[i],indices.numpy()[j]])\n",
    "\n",
    "def fun(x,asset_input,y):\n",
    "    return heston(S0,V0,r,x,asset_input) - y\n",
    "\n",
    "\n",
    "\n",
    "x0 = np.array([-0.3, 0.03, 1.3, 0.3])\n",
    "#res_lsq = least_squares(fun, x0, args=(asset_input, target.ravel().numpy()))\n",
    "#heston_info = res_lsq.x\n",
    "heston_info = [0.75,0.89,2.23,0.3]\n",
    "print( \"*\"*20,\"The calibrated heston model params: \\\n",
    "        rho = {},theta = {}, kappa = {} and lambda = {}\".format(heston_info[0],heston_info[1],\\\n",
    "        heston_info[2],heston_info[3]),\"*\"*20)\n",
    "\n",
    "\n",
    "model_pro = Net_SDE_Pro(heston_info,asset_info,3,timegrid,\n",
    "                        n_layers=2,vNetWidth = 20,device=device)\n",
    "\n",
    "print(\"===\"*10+\"training the neural sde pro model\"+\"===\"*10)\n",
    "model_pro,losses_pro, losses_val_pro=train_models(model_pro,target,n_steps,option_info ,indices,MC_samples)\n",
    "\n",
    "\n",
    "\n",
    "z_1 = np.random.normal(size=(MC_samples, n_steps))\n",
    "z_2 = np.random.normal(size=(MC_samples, n_steps))\n",
    "z_1 = np.append(z_1,-z_1,axis=0)\n",
    "z_2 = np.append(z_2,-z_2,axis=0)\n",
    "z_2  = rho*z_1+np.sqrt(1-rho ** 2)*z_2\n",
    "z_1 = torch.tensor(z_1).to(device=device).float()\n",
    "z_2 = torch.tensor(z_2).to(device=device).float()\n",
    "\n",
    "\n",
    "pred = model(strikes_call, strikes_put,indices, z_1,z_2, 2*MC_samples).detach()\n",
    "pred_pro = model_pro(strikes_call, strikes_put,indices, z_1,z_2, 2*MC_samples).detach()\n",
    "\n",
    "loss_fn = nn.L1Loss() \n",
    "print(\"The train loss for nsde:    \",loss_fn(pred,target))\n",
    "print(\"The train loss for nsde pro:    \",loss_fn(pred_pro,target))\n",
    "\n",
    "strikes_call_test = np.arange(60, 101, 2.5).tolist()\n",
    "strikes_put_test = np.arange(100, 141, 2.5).tolist()\n",
    "pred_test = model(strikes_call_test, strikes_put_test,indices, z_1,z_2, 2*MC_samples).detach()\n",
    "pred_pro_test = model_pro(strikes_call_test, strikes_put_test,indices, z_1,z_2, 2*MC_samples).detach()\n",
    "\n",
    "ITM_call_test = torch.Tensor(torch.load(data_path+'Call_ITM_VG_test.pt')).to(device=device)\n",
    "OTM_call_test = torch.Tensor(torch.load(data_path+'Call_OTM_VG_test.pt')).to(device=device)\n",
    "target_test = torch.cat([OTM_call_test, ITM_call_test],0)\n",
    "print(loss_fn(pred_test,target_test))\n",
    "print(loss_fn(pred_pro_test,target_test))\n",
    "\n",
    "nsde_test_loss = (loss_fn(pred_test,target_test)*target_test.shape[0]*target_test.shape[1]\n",
    "                  -loss_fn(pred,target)*target.shape[0]*target.shape[1])/\\\n",
    "                 (target_test.shape[0]*target_test.shape[1]-target.shape[0]*target.shape[1])\n",
    "nsde_pro_test_loss = (loss_fn(pred_pro_test,target_test)*target_test.shape[0]*target_test.shape[1]\n",
    "                  -loss_fn(pred_pro,target)*target.shape[0]*target.shape[1])/\\\n",
    "                 (target_test.shape[0]*target_test.shape[1]-target.shape[0]*target.shape[1])\n",
    "\n",
    "print(\"The test loss for nsde:  \",nsde_test_loss)\n",
    "print(\"The test loss for nsde pro:  \", nsde_pro_test_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.4845, 39.0501, 36.6183, 34.1909, 31.7703, 29.3606, 26.9672, 24.5965,\n",
       "        22.2588, 19.9649, 17.7304, 15.5693, 13.5011, 11.5461,  9.7222,  8.0487,\n",
       "         6.5426])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test[19,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.4528, 36.5865, 31.7390, 26.9371, 22.2315, 17.7057, 13.4794,  9.7002,\n",
       "         6.5225])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[19,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.9039, 37.0613, 32.2189, 27.3825, 22.5878, 17.9360, 13.5888,  9.7224,\n",
       "         6.5015])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[19,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5015e+00, 5.1676e+00, 4.0079e+00, 3.0326e+00, 2.2409e+00, 1.6095e+00,\n",
       "        1.1300e+00, 7.6735e-01, 5.0543e-01, 3.2122e-01, 1.9895e-01, 1.1825e-01,\n",
       "        6.8725e-02, 3.9120e-02, 2.0239e-02, 9.8557e-03, 4.5573e-03])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test[19,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 17])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 65, 70, 75, 80, 85, 90, 95, 100]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strikes_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.0,\n",
       " 62.5,\n",
       " 65.0,\n",
       " 67.5,\n",
       " 70.0,\n",
       " 72.5,\n",
       " 75.0,\n",
       " 77.5,\n",
       " 80.0,\n",
       " 82.5,\n",
       " 85.0,\n",
       " 87.5,\n",
       " 90.0,\n",
       " 92.5,\n",
       " 95.0,\n",
       " 97.5,\n",
       " 100.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strikes_call_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d769be7eb8497c6778dab04ea1ae8cec9ddf20b31fc10cf26ad01d9416e8fdb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pycourse': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
